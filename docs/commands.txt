Commands for reference:

git clone https://github.com/hashcerts/kubernetes-devops-security.git
bash kubernetes-devops-security/setup/vm-install-script/install-script.sh

https://www.digitalocean.com/community/tutorials/how-to-install-jenkins-on-ubuntu-20-04
https://askubuntu.com/questions/660599/i-am-installing-jenkins-server-but-its-giving-w-gpg-error
JENKINS
=======
##### apt-get -y install openjdk-8-jdk

wget -q -O - https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo apt-key add -
sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ > /etc/apt/sources.list.d/jenkins.list'
sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 5BA31D57EF5975CA
sudo apt update
sudo apt install jenkins

START
=====
sudo systemctl start jenkins
sudo systemctl status jenkins



kubectl run nginx-pod --image nginx
kubectl get po


KUBE VERIFICATION
=================
kubectl get node -o wide


# Jenkins plugins
bash installer.sh

apt update


KUBECONFIG
==========
cd ~
cat.kube/config

WEB HOOK INTEGRATION


DOCKER HUB INTEGRATION
======================
#https://docs.oracle.com/cd/E19253-01/816-4557/secfile-60/index.html#:~:text=When%20you%20create%20a%20file,and%20execute%20permission%20to%20everyone.
chmod 666 docker.sock


kubectl -n default create deploy node-app --image hashcerts/node-service:v1 
kubectl -n default expose deploy node-app --name node-service --port 5000



## Talisman Installation
curl https://thoughtworks.github.io/talisman/install.sh > ~/install-talisman.sh
chmod +x ~/install-talisman.sh
cd kubernetes-devops-security/
~/install-talisman.sh



## Create sec_files
mkdir sec_files && cd sec_files
echo "username=hashcerts" > file1
echo "secure-password123" > password.txt
echo "apikey=AizaSyCqhjgrPtr_La56sdUkjfav_laCqhjgrPtr_2s" > file2
echo "base64encodedsecret=cGFzc3dvcmQtaXMtcXdlcnR5MTIzCg==" > file3



git add / push / pull





## Run SonarQube as a Docker Container
docker run -d --name sonarqube -e SONAR_ES_BOOTSTRAP_CHECKS_DISABLE=true -p 9000:9000 sonarqube:8.9.1-community


docker ps -a | grep -i sonar

docker restart sonarcube [after every single restart of azure virtual machine]
or
docker start 6f8e35cfcb5f [use id instead of the name if the above restart command doesn't work]

# to run the sonarqube SAST
mvn sonar:sonar \
  -Dsonar.projectKey=numeric-application \
  -Dsonar.host.url=http://devsecops-demoez.eastus.cloudapp.azure.com:9000 \
  -Dsonar.login=fdc69c2d3c1182325a559b5445b6b34902961467
  
  
  
## Trivy
docker run --rm -v $HOME/Library/Caches:/root/.cache aquasec/trivy:0.18.3 python:3.4-alpine

docker run --rm -v $HOME/Library/Caches:/root/.cache aquasec/trivy:0.18.3 --severity CRITICAL python:3.4-alpine


# to print the exit code
echo $?


docker run --rm -v $HOME/Library/Caches:/root/.cache aquasec/trivy:0.18.3 --severity CRITICAL --exit-code 1 python:3.4-alpine

docker run --rm -v $HOME/Library/Caches:/root/.cache aquasec/trivy:0.18.3 --severity LOW --exit-code 0 python:3.4-alpine



# check for vulnerability on the command line
docker run --rm -v $WORKSPACE:/root/.cache/ aquasec/trivy:0.17.2 -q image --exit-code 1 --severity CRITICAL --light openjdk

docker run --rm -v $WORKSPACE:/root/.cache/ aquasec/trivy:0.17.2 -q image --exit-code 1 --severity CRITICAL --light openjdk:8

docker run --rm -v $WORKSPACE:/root/.cache/ aquasec/trivy:0.17.2 -q image --exit-code 1 --severity CRITICAL --light openjdk:8-alpine

# recommended version to be used for Trivy
docker run --rm -v $WORKSPACE:/root/.cache/ aquasec/trivy:0.17.2 -q image --exit-code 1 --severity CRITICAL --light adoptopenjdk/openjdk8:alpine-slim 


# To find the user for the container / pod
kubectl get po
kubectl exec -it devsecops-b4669c6b7-j2c6w -- id


# To list the number of users
cat /etc/passwd | wc -l
# To show the list of users
cat /etc/passwd



# Kubernetes OPA Conftest
kubectl get deploy

# Rollout
kubectl rollout deploy -h

# Rollback
kubectl rollout history deploy devsecops

kubectl rollout status deploy devsecops


# To find out the status of pod
kubectl describe po

# To get the list of deployment
kubectl get depoyment

# To undo or rollback a deployment
kubectl rollout undo deploy devsecops


# To get the id of the container pod
kubectl exec -it devsecops-7455fd6b96-kztw7 -- id

# Generate ZAP ConfigFile to add ignore rules
docker run -v $(pwd):/zap/wrk/:rw -t owasp/zap2docker-weekly zap-api-scan.py -t http://devsecops-demoez.eastus.cloudapp.azure.com:32270/v3/api-docs -f openapi -g gen_file


# Generate ZAP ConfigFile to copy the newly added ignore rules
docker run -v $(pwd):/zap/wrk/:rw -t owasp/zap2docker-weekly zap-api-scan.py -t http://devsecops-demoez.eastus.cloudapp.azure.com:32270/v3/api-docs -f openapi -c gen_file







Section 4
Kubernetes Operations and Security

docker run --pid=host -v /etc:/etc:ro -v /var:/var:ro -v $(which kubectl):/usr/local/mount-from-host/bin/kubectl -v ~/.kube:/.kube -e KUBECONFIG=/.kube/config -t aquasec/kube-bench:latest  run --targets master --version 1.19 --check 1.2.7,1.2.8,1.2.9 --json | jq .Totals.total_fail


Kube-bench
==========
https://github.com/aquasecurity/kube-bench/blob/main/docs/installation.md

curl -L https://github.com/aquasecurity/kube-bench/releases/download/v0.6.2/kube-bench_0.6.2_linux_amd64.tar.gz -o kube-bench_0.6.2_linux_amd64.tar.gz

tar -xvf kube-bench_0.6.2_linux_amd64.tar.gz

kube-bench

cat /var/lib/kubelet/config.yaml
./kube-bench --config-dir `pwd`/cfg --config `pwd`/cfg/config.yaml 

./kube-bench master --config-dir `pwd`/cfg --config `pwd`/cfg/config.yaml

# node will be deprecated
./kube-bench node --check 4.2.1 --config-dir `pwd`/cfg --config `pwd`/cfg/config.yaml --json 

# use instead
./kube-bench run --check 4.2.1 --config-dir `pwd`/cfg --config `pwd`/cfg/config.yaml --json 

./kube-bench node --check 4.2.1 --config-dir `pwd`/cfg --config `pwd`/cfg/config.yaml --json 

# piping to jq for formatting
./kube-bench run --check 4.2.1 --config-dir `pwd`/cfg --config `pwd`/cfg/config.yaml --json | jq .[].total_pass

systemctl restart kubelet

ps -ef | grep kubelet


mTLS - ISTIO
====
################### Istio Installation ################### 

curl -Ls https://istio.io/downloadIstio | ISTIO_VERSION=1.9.0 sh -
cd istio-1.9.0
export PATH=$PWD/bin:$PATH
istioctl install --set profile=demo -y && kubectl apply -f samples/addons

################### Istio Installation ################### 


#Namespace for Istio
kubectl -n istio-system get all


kubectl get namespaces

kubectl -n istio-system edit svc kiali


# formatting sonarqube results in json format
docker ps --format '{"ID":"{{ .ID }}", "Image": "{{ .Image }}", "Names":"{{ .Names }}"}' | grep "sonarqube" -i

# formatting sonarqube results in json format beautified
docker ps --format '{"ID":"{{ .ID }}", "Image": "{{ .Image }}", "Names":"{{ .Names }}"}' | grep "sonarqube" -i |  jq .

# formatting sonarqube results in json format beautified and to fetch ID only from the results



# https://stackoverflow.com/questions/49976188/copy-docker-image-between-repositories
# docker images pull from one repository and pushing to another one
docker pull siddharth67/node-service:v1
docker tag siddharth67/node-service:v1 hashcerts/node-service:v1
docker push hashcerts/node-service:v1

# in case if docker account isn't available or access permission is denied
https://forums.docker.com/t/docker-push-error-requested-access-to-the-resource-is-denied/64468


# istio pod to pod communication section
kubectl get ns
kubectl create ns prod
docker images | grep -i node-service
kubectl -n prod create deploy node-app --image hashcerts/node-service:v1
kubectl -n prod expose deploy node-app --name node-service --port 5000

kubectl get all -n prod

kubectl get ns --show-labels

kubectl label ns prod istio-injection=enabled

kubectl label ns prod istio-injection=enabled --overwrite=true.

kubectl get po

kubectl get po -n prod
kubectl -n prod get po



kubectl -n prod rollout

kubectl -n prod rollout restart deploy node-app

kubectl get po -n prod


kubectl -n prod describe po node-app-7bb5f79689-77tst




# while syntax Kiali
while true; do curl -s localhost:32270/increment/99; echo leep 1; done
while true; do curl -s 10.100.27.145:8080/increment/99; echo ; sleep .1; done



enable/disable mTLS between pod -- Istio
========================================
kubectl apply -f peer-auth.yaml -n istio-system


Ingress Gateway
===============
kubectl get crd
kubectl apply -f istio-gateway-vs.yaml
kubectl get vs,gateway -n prod
kubectl -n istio-system get svc
kubectl -n istio-system get svc istio-ingressgateway
curl localhost:31536/
curl localhost:31536/increment/11

curl localhost:31536/ -v

kubectl get pa -A


kubectl -n istio-system get svc

# change from cluster ip to nodeport
kubectl -n istio-system edit svc prometheus
kubectl -n istio-system edit svc grafana

kubectl -n istio-system get svc

kubectl -n prod get svc

kubectl get pa -A


Falco
=====
https://falco.org/docs/install-operate/installation/

curl -fsSL https://falco.org/repo/falcosecurity-packages.asc | \
  sudo gpg --dearmor -o /usr/share/keyrings/falco-archive-keyring.gpg

echo "deb [signed-by=/usr/share/keyrings/falco-archive-keyring.gpg] https://download.falco.org/packages/deb stable main" | \
sudo tee -a /etc/apt/sources.list.d/falcosecurity.list

sudo apt-get update -y

sudo apt install -y dkms make linux-headers-$(uname -r)
# If you use falcoctl driver loader to build the eBPF probe locally you need also clang toolchain
sudo apt install -y clang llvm
# You can install also the dialog package if you want it
sudo apt install -y dialog

sudo apt-get install -y falco

-------------------------------

journalctl -u falco -f
